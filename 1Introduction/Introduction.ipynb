{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Aujourd'hui l'or noir des entreprises est la donnée. Il existe une\n",
    "infinité de sources potentiellement récupérables et utilisables:\n",
    "\n",
    "-   Données propriétaires (Données clients, Données d'exploitation,\n",
    "    Données de capteurs...)\n",
    "-   Données Open Data (Adresses, Démographie des départements ou\n",
    "    villes...)\n",
    "-   Données publiques disponibles sur le web (Tweets, Réseaux sociaux,\n",
    "    ...)\n",
    "\n",
    "Dans la quête de Data de chaque entreprise, l'important est d'agréger et\n",
    "de mettre en relation un grand nombre de données différentes pour\n",
    "pouvoir en tirer le maximum d'informations et mettre en place des axes\n",
    "d'amélioration.\n",
    "\n",
    "## Présentation du cours\n",
    "\n",
    "Le but de ce cours est de présenter les tenants et les aboutissants\n",
    "d'une infrastucture de récupération de données.\n",
    "\n",
    "### Contexte de Qwant\n",
    "\n",
    "Qwant est un moteur de recherche européen basé sur un concept fort de\n",
    "vie privée. Nous ne gardons pas les informations utilisateurs. Pour\n",
    "avoir quelques chiffres: - Nous avons actuellement un ferme de 750\n",
    "crawlers qui permettent de récupérer environ 1500 pages secondes soit\n",
    "5,4M de pages par heure. - Le web francais est estimé à 150M de pages ce\n",
    "qui représente environ 2 Peta Bytes de données duppliquées.\n",
    "\n",
    "## Quelques mots clés et définitions\n",
    "\n",
    "### Crawler\n",
    "\n",
    "Un Crawler est un robot qui permet de récupérer des données\n",
    "non-structurées soit des informations textuelles, structurelles et de\n",
    "contenu d'un site web. La structure de l'algorithme utilisé doit être\n",
    "agnostique de la structure HTML du site web. Elle permet de récupérer\n",
    "des informations de base comme le texte, les images, les liens entrants\n",
    "ou sortants par exemple.\n",
    "\n",
    "### Scraper\n",
    "\n",
    "Un Scraper est dépendant du site et de sa structure. Il permet de\n",
    "récupérer des informations beaucoup plus qualitatives sur un site web.\n",
    "Les Scraper ne sont pas très facilement maintenables puisqu'ils sont\n",
    "basés sur la structure HTML qui est vouée à changer plus ou moins\n",
    "rapidement selon les sites.\n",
    "\n",
    "## Les bonnes pratiques\n",
    "\n",
    "On peut comprendre très rapidemement si les sites et les webmasters ont\n",
    "envie qu'on puisse accéder à leurs données. Plusieurs manières\n",
    "permettent de montrer ou d'expliciter les comportements non recommandés\n",
    "sur un site.\n",
    "\n",
    "Aujourd'hui certains sites utilisent des méthodes pour empecher la\n",
    "récupération massive de leurs données :\n",
    "\n",
    "-   Génération à la volée de code HTML et CSS. Le nom des balises HTML\n",
    "    est générée de facon à ce qu'on ne puisse pas se baser sur\n",
    "    celles-ci.\n",
    "-   Black list d'adresses IP détectées.\n",
    "-   Génération de contenu via du JavaScript\n",
    "-   Algorithmes de détection de comportements non-humains (vitesse de\n",
    "    navigation, scroll, click, etc)\n",
    "\n",
    "Plusieurs méthodes sont possibles pour éviter ou contourner ces\n",
    "limitations mais elles ne seront pas abordées dans ce cours.\n",
    "\n",
    "### Robots.txt\n",
    "\n",
    "La limitation la plus simple et la plus connue est le fichier\n",
    "Robots.txt. Il est édité par les webmasters des sites pour \"contrôler\"\n",
    "le comportement des robots sur leur site. Différentes politiques sont\n",
    "mises en place par les organismes en fonction des problématiques\n",
    "métiers. Ce fichier n'empêche absolument pas de récupérer les données\n",
    "mais fait part d'une bonne pratique.\n",
    "\n",
    "-   <https://www.google.com/robots.txt>\n",
    "-   <http://www.seloger.com/robots.txt>\n",
    "-   <https://www.leboncoin.fr/robots.txt>\n",
    "-   <https://booking.com/robots.txt>\n",
    "\n",
    "### Site Map ou Site Index\n",
    "\n",
    "Le site map ou le site index (plan du site) sont des pages HTML générées\n",
    "pour améliorer le SEO d'une page. Le SEO (Search Engine Optimisation)\n",
    "permet d'optimiser le référencement sur les moteurs de recherche. La\n",
    "plupart des gros sites ont des équipes SEO dédiées qui permettent aux\n",
    "sites d'être présents dans les premières positions lors des recherches\n",
    "associées. Ces pages donnent accès à l'arbre de génération ou de\n",
    "structure du site. La plupart du temps elles permettent l'exploration\n",
    "massive et facile des sites pour les robots de crawl des moteurs de\n",
    "recherche.\n",
    "\n",
    "### Surcharge du serveur\n",
    "\n",
    "La plupart des sites importants ont des infrastructures qui tiennent la\n",
    "charge et qui peuvent être utilisées et appelées un très grand nombre de\n",
    "fois. D'autres sont beaucoup plus restreintes et donc il est important\n",
    "de ne pas surcharger celles-ci. Les sites comme Wikipédia ou\n",
    "StackOverFlow empêchent les robots d'accéder trop rapidement à leurs\n",
    "infrastructures et forcent des temps d'arrêt entre la récupération des\n",
    "différentes pages.\n",
    "\n",
    "## Introduction au scraping\n",
    "\n",
    "Il existe deux grandes pratiques pour scraper un site efficacement. Nous\n",
    "allons aborder les deux :\n",
    "\n",
    "-   Récupération et parsing du code HTML. Cette solution nécessite une\n",
    "    compréhension du code et des notions basiques de DOM et architecture\n",
    "    HTML.\n",
    "-   Récupération des appels API aux serveurs permettant de récupérer les\n",
    "    informations directement à la source, la plupart du temps au format\n",
    "    JSON. Cette deuxième solution est la plus efficace et facile mais\n",
    "    les appels d'API sont souvent cachés ou bloqués.\n",
    "\n",
    "Dans les deux cas, nous utiliserons des requêtes HTTP et le package\n",
    "`requests`. Celui-ci permet de faire des requêtes très rapidement et\n",
    "facilement via un interpreteur Python. De nombreux paramètres sont\n",
    "modifiables.\n",
    "\n",
    "Pour réaliser ces opérations une bonne pratique est d'utiliser l'outil\n",
    "de developpement de Chrome ou Firefox. Je conseille celui de Google qui\n",
    "est beaucoup plus intuitif et développé que celui de Mozilla. Il existe\n",
    "plusieurs raccourcis claviers mais le plus simple est de faire une click\n",
    "droit et `inspecter`.\n",
    "\n",
    "![image](images/inspecteur.png)\n",
    "\n",
    "Deux onglets sont importants dans notre cas :\n",
    "\n",
    "-   `Element` : la partie correspondant au code HTML : elle permet de\n",
    "    visualiser la structure et répérer les pointeurs des balises qui\n",
    "    encapsulent nos données.\n",
    "- `Network` : cette partie permet d'analyser tous les appels réseaux\n",
    "réalisés depuis le front. C'est ici que les appels de récupération de\n",
    "données sont effectués.\n",
    "\n",
    "\n",
    "\n",
    "# Architecture\n",
    "\n",
    "Il est important en développement logiciel de réfléchir directement à\n",
    "une infrastructure solide et scalable. Cela demande un coût de\n",
    "développement plus important mais un coût de maintenance beaucoup plus\n",
    "restreint.\n",
    "\n",
    "Plusieurs technologies peuvent être utilisées en fonction des besoins et\n",
    "des affinités de chacun.\n",
    "\n",
    "## Récupération des données\n",
    "\n",
    "Pour la récupération des données n'importe quelle technologie peut être\n",
    "utilisée. Il suffit de pouvoir faire des requêtes HTTP et de pouvoir\n",
    "parser le résultat. Tous les langages de programmation possèdent des API\n",
    "pour faire ces transformations. En python de nombreuses librairies ont\n",
    "été développée dans ce but et nous en aborderons plusieurs dans ce cours\n",
    ":\n",
    "\n",
    "-   Requests (+ requests\\_cache qui fait gagner un temps précieux lors\n",
    "    des développements)\n",
    "-   BeautifulSoup (permet de créer un objet python à partir de code HTML\n",
    "    pour faciliter l'extraction)\n",
    "-   Readability (permet de récupérer le texte pertinent d'une page web)\n",
    "-   Scrapy (Framework de Spider permettant de gérer le Crawling et le\n",
    "    Scraping de site web très efficacement)\n",
    "\n",
    "## Stockage\n",
    "\n",
    "Pour le stockage un grand nombre de bases de données sont disponibles\n",
    "sur le marché. Chacune ayant sa particularité, il est souhaitable de\n",
    "bien définir les besoins et le format des données pour utiliser la base\n",
    "la plus adaptée à son environnement. Dans le contexte du web, les bases\n",
    "noSQL sont très recherchées et demandées, à cause de leur structure très\n",
    "flexible et optimisée pour le stockage de données hétérogènes. Quelques\n",
    "examples de bases de données :\n",
    "\n",
    "-   MySQL (Base de données relationnelles, stockage sous forme de\n",
    "    tableau, optimale pour le stockage de chiffres) ;\n",
    "-   PostGreSQL (Deuxième base de données relationnelles, avec une\n",
    "    surcouche d'optimisation pour les données géolocalisées) ;\n",
    "-   Cassandra (Base de données noSQL developpée pour garantir une\n",
    "    scalabilité et intégrité d'un grand nombre de données) ;\n",
    "-   MongoDB (Base de données noSQL, stockage sous la forme de document\n",
    "    JSON)\n",
    "-   Redis (Base de données noSQL, stockage sous la forme de données\n",
    "    clé:valeurs avec très grandes performances)\n",
    "\n",
    "\\* ElasticSearch n'est pas réellement une base de données, c'est un\n",
    "moteur de recherche qui permet de faire de la recherche très\n",
    "efficacement dans des données textuelles, numériques et géolocalisées.\n",
    "\n",
    "## Scalabilité\n",
    "\n",
    "Tout dépend de ce dont vous avez besoin mais souvent l'extraction sur\n",
    "une seule machine et un seul processus s'avérera trop lente et peu\n",
    "efficace. Il existe des outils permettant de distribuer la charge de\n",
    "calcul. Pour ce faire, une queue de messages est créée, ce qui permet\n",
    "d'envoyer les instructions à différents programmes devant les exécuter.\n",
    "Les deux les plus utilisés sont :\n",
    "\n",
    "-   RabbitMQ : développé initialement pour l'internet des objets et\n",
    "    l'échange de données entre objets connectés.\n",
    "-   Kafka : développé par les équipes de LinkedIn pour faire du\n",
    "    streaming de données et partager les données avec différentes\n",
    "    interfaces.\n",
    "\n",
    "## Exemple d'Architecture\n",
    "\n",
    "<img src=\"images/architecture_globale.png\" alt=\"Global Architecture\" class=\"align-right\" width=\"700\" height=\"50\" />\n",
    "\n",
    "# Suite\n",
    "\n",
    "Ouvrez un navigateur et allez à l'adresse <http://localhost:8888>\n",
    "\n",
    "Allez directement voir le notebook :\n",
    "- [Part1_Simple_Web_Scraping.ipynb](http://localhost:8889/notebooks/1Introduction/Part1_Simple_Web_Scraping.ipynb)\n",
    "- [Part2_Web_Scraping_WebDriver.ipynb](http://localhost:8889/notebooks/1Introduction/Part2_Web_Scraping_WebDriver.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
